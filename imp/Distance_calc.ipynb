{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHV_KqIRCMAd",
        "outputId": "8511f37f-2ad4-41f5-d6e5-1e4f74392947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGCI0_JRS9jb",
        "outputId": "59ebd38c-a6dc-46de-b34e-843b0881f111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.29.35)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (0.12.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.5.3)\n",
            "Collecting timm (from -r requirements.txt (line 15))\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->-r requirements.txt (line 10)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->-r requirements.txt (line 10)) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->-r requirements.txt (line 10)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->-r requirements.txt (line 10)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->-r requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->-r requirements.txt (line 10)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->-r requirements.txt (line 10)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.1->-r requirements.txt (line 10)) (16.0.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.10.1->-r requirements.txt (line 11)) (2.27.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 14)) (2022.7.1)\n",
            "Collecting huggingface-hub (from timm->-r requirements.txt (line 15))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm->-r requirements.txt (line 15))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->-r requirements.txt (line 15)) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->-r requirements.txt (line 10)) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.10.1->-r requirements.txt (line 11)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.10.1->-r requirements.txt (line 11)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.10.1->-r requirements.txt (line 11)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.10.1->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->-r requirements.txt (line 10)) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/Pandas-Team/Autonomous-Vehicle-Environment-Perception.git\n",
        "%cd /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5CfbbmYH2Oz",
        "outputId": "d3f3eab7-f92a-4ac4-8ee7-5c43469140f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAutonomous-Vehicle-Environment-Perception\u001b[0m/  main.py           \u001b[01;34mvideos\u001b[0m/\n",
            "classes.txt                                 \u001b[01;34moutputs\u001b[0m/          \u001b[01;34mweights\u001b[0m/\n",
            "\u001b[01;34melements\u001b[0m/                                   \u001b[01;34mPINet\u001b[0m/            \u001b[01;34myolov5\u001b[0m/\n",
            "\u001b[01;34mimgs\u001b[0m/                                       README.md         yolov5s.pt\n",
            "Instructions.ipynb                          requirements.txt\n",
            "LICENSE                                     \u001b[01;34mSGDepth\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynu0F7JH5pFN",
        "outputId": "c9424a07-4ecd-40c1-fec0-ed6115a55f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.009s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "# import unittest\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.autograd as autograd\n",
        "\n",
        "\n",
        "# class ScaleGrad(autograd.Function):\n",
        "#     @staticmethod\n",
        "#     def forward(ctx, scale, *inputs):\n",
        "#         ctx.scale = scale\n",
        "\n",
        "#         outputs = inputs[0] if (len(inputs) == 1) else inputs\n",
        "\n",
        "#         return outputs\n",
        "\n",
        "#     @staticmethod\n",
        "#     def backward(ctx, *grad_outputs):\n",
        "#         grad_inputs = tuple(\n",
        "#             ctx.scale * grad\n",
        "#             for grad in grad_outputs\n",
        "#         )\n",
        "\n",
        "#         return (None, *grad_inputs)\n",
        "\n",
        "\n",
        "# class ScaledSplit(nn.Module):\n",
        "#     \"\"\"Identity maps an input into outputs and scale gradients in the backward pass\n",
        "\n",
        "#     Args:\n",
        "#         *grad_weights: one or multiple weights to apply to the gradients in\n",
        "#             the backward pass\n",
        "\n",
        "#     Examples:\n",
        "\n",
        "#         >>> # Multiplex to two outputs, the gradients are scaled\n",
        "#         >>> # by 0.3 and 0.7 respectively\n",
        "#         >>> scp = ScaledSplit(0.3, 0.7)\n",
        "#         >>> # The input may consist of multiple tensors\n",
        "#         >>> inp\n",
        "#         (tensor(...), tensor(...))\n",
        "#         >>> otp1, otp2 = scp(inp)\n",
        "#         >>> # Considering the forward pass both outputs behave just like inp.\n",
        "#         >>> # In the backward pass the gradients will be scaled by the respective\n",
        "#         >>> # weights\n",
        "#         >>> otp1\n",
        "#         (tensor(...), tensor(...))\n",
        "#         >>> otp2\n",
        "#         (tensor(...), tensor(...))\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, *grad_weights):\n",
        "#         super().__init__()\n",
        "#         self.set_scales(*grad_weights)\n",
        "\n",
        "#     def set_scales(self, *grad_weights):\n",
        "#         self.grad_weights = grad_weights\n",
        "\n",
        "#     def get_scales(self, *grad_weights):\n",
        "#         return self.grad_weights\n",
        "\n",
        "#     def forward(self, *inputs):\n",
        "#         # Generate nested tuples, where the outer layer\n",
        "#         # corresponds to the output & grad_weight pairs\n",
        "#         # and the inner layer corresponds to the list of inputs\n",
        "#         split = tuple(\n",
        "#             tuple(ScaleGrad.apply(gw, inp) for inp in inputs)\n",
        "#             for gw in self.grad_weights\n",
        "#         )\n",
        "\n",
        "#         # Users that passed only one input don't expect\n",
        "#         # a nested tuple as output but rather a tuple of tensors,\n",
        "#         # so unpack if there was only one input\n",
        "#         unnest_inputs = tuple(\n",
        "#             s[0] if (len(s) == 1) else s\n",
        "#             for s in split\n",
        "#         )\n",
        "\n",
        "#         # Users that specified only one output weight\n",
        "#         # do not expect a tuple of tensors put rather\n",
        "#         # a single tensor, so unpack if there was only one weight\n",
        "#         unnest_outputs = unnest_inputs[0] if (len(unnest_inputs) == 1) else unnest_inputs\n",
        "\n",
        "#         return unnest_outputs\n",
        "\n",
        "\n",
        "# class GRL(ScaledSplit):\n",
        "#     \"\"\"Identity maps an input and invert the gradient in the backward pass\n",
        "\n",
        "#     This layer can be used in adversarial training to train an encoder\n",
        "#     encoder network to become _worse_ a a specific task.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(-1)\n",
        "\n",
        "\n",
        "# class TestScaledSplit(unittest.TestCase):\n",
        "#     def test_siso(self):\n",
        "#         factor = 0.5\n",
        "\n",
        "#         scp = ScaledSplit(factor)\n",
        "\n",
        "#         # Construct a toy network with inputs and weights\n",
        "#         inp = torch.tensor([ 1, 1, 1], dtype=torch.float32, requires_grad=True)\n",
        "#         wgt = torch.tensor([-1, 0, 1], dtype=torch.float32, requires_grad=False)\n",
        "\n",
        "#         pre_split = inp * wgt\n",
        "#         post_split = scp.forward(pre_split)\n",
        "\n",
        "#         self.assertTrue(torch.equal(pre_split, post_split), 'ScaledSplit produced non-identity in forward pass')\n",
        "\n",
        "#         # The network's output is a single number\n",
        "#         sum_pre = pre_split.sum()\n",
        "#         sum_post = post_split.sum()\n",
        "\n",
        "#         # Compute the gradients with and withou scaling\n",
        "#         grad_pre, = autograd.grad(sum_pre, inp, retain_graph=True)\n",
        "#         grad_post, = autograd.grad(sum_post, inp)\n",
        "\n",
        "#         # Check if the scaling matches expectations\n",
        "#         self.assertTrue(torch.equal(grad_pre * factor, grad_post), 'ScaledSplit produced inconsistent gradient')\n",
        "\n",
        "#     def test_simo(self):\n",
        "#         # TODO\n",
        "\n",
        "#         pass\n",
        "\n",
        "#     def test_miso(self):\n",
        "#         # TODO\n",
        "\n",
        "#         pass\n",
        "\n",
        "\n",
        "#     def test_mimo(self):\n",
        "#         # TODO\n",
        "\n",
        "#         pass\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "pjLNPAIr5nAa",
        "outputId": "25cd2eae-de50-417b-a92a-3ebbdd7a252e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b5f2648b4b79>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# from . import networks\n",
        "# from . import layers\n",
        "\n",
        "\n",
        "# class SGDepthCommon(nn.Module):\n",
        "#     def __init__(self, num_layers, split_pos, grad_scales=(0.9, 0.1), pretrained=False):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.encoder = networks.ResnetEncoder(num_layers, pretrained)\n",
        "#         self.num_layers = num_layers  # This information is needed in the train loop for the sequential training\n",
        "\n",
        "#         # Number of channels for the skip connections and internal connections\n",
        "#         # of the decoder network, ordered from input to output\n",
        "#         self.shape_enc = tuple(reversed(self.encoder.num_ch_enc))\n",
        "#         self.shape_dec = (256, 128, 64, 32, 16)\n",
        "\n",
        "#         self.decoder = networks.PartialDecoder.gen_head(self.shape_dec, self.shape_enc, split_pos)\n",
        "#         self.split = layers.ScaledSplit(*grad_scales)\n",
        "\n",
        "#     def set_gradient_scales(self, depth, segmentation):\n",
        "#         self.split.set_scales(depth, segmentation)\n",
        "\n",
        "#     def get_gradient_scales(self):\n",
        "#         return self.split.get_scales()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # The encoder produces outputs in the order\n",
        "#         # (highest res, second highest res, …, lowest res)\n",
        "#         x = self.encoder(x)\n",
        "\n",
        "#         # The decoder expects it's inputs in the order they are\n",
        "#         # used. E.g. (lowest res, second lowest res, …, highest res)\n",
        "#         x = tuple(reversed(x))\n",
        "\n",
        "#         # Replace some elements in the x tuple by decoded\n",
        "#         # tensors and leave others as-is\n",
        "#         x = self.decoder(*x) # CHANGE ME BACK TO THIS\n",
        "\n",
        "#         # Setup gradient scaling in the backward pass\n",
        "#         x = self.split(*x)\n",
        "\n",
        "#         # Experimental Idea: We want the decoder layer to be trained, so pass x to the decoder AFTER x was passed\n",
        "#         # to self.split which scales all gradients to 0 (if grad_scales are 0)\n",
        "#         # x = (self.decoder(*x[0]), ) + (self.decoder(*x[1]), ) + (self.decoder(*x[2]), )\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     #def get_last_shared_layer(self):\n",
        "#     #    return self.encoder.encoder.layer4\n",
        "\n",
        "\n",
        "# class SGDepthDepth(nn.Module):\n",
        "#     def __init__(self, common, resolutions=1):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.resolutions = resolutions\n",
        "\n",
        "#         self.decoder = networks.PartialDecoder.gen_tail(common.decoder)\n",
        "#         self.multires = networks.MultiResDepth(self.decoder.chs_x()[-resolutions:])\n",
        "\n",
        "#     def forward(self, *x):\n",
        "#         x = self.decoder(*x)\n",
        "#         x = self.multires(*x[-self.resolutions:])\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class SGDepthSeg(nn.Module):\n",
        "#     def __init__(self, common):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.decoder = networks.PartialDecoder.gen_tail(common.decoder)\n",
        "#         self.multires = networks.MultiResSegmentation(self.decoder.chs_x()[-1:])\n",
        "#         self.nl = nn.Softmax2d()\n",
        "\n",
        "#     def forward(self, *x):\n",
        "#         x = self.decoder(*x)\n",
        "#         x = self.multires(*x[-1:])\n",
        "#         x_lin = x[-1]\n",
        "\n",
        "#         return x_lin\n",
        "\n",
        "\n",
        "# class SGDepthPose(nn.Module):\n",
        "#     def __init__(self, num_layers, pretrained=False):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.encoder = networks.ResnetEncoder(\n",
        "#             num_layers, pretrained, num_input_images=2\n",
        "#         )\n",
        "\n",
        "#         self.decoder = networks.PoseDecoder(self.encoder.num_ch_enc[-1])\n",
        "\n",
        "#     def _transformation_from_axisangle(self, axisangle):\n",
        "#         n, h, w = axisangle.shape[:3]\n",
        "\n",
        "#         angles = axisangle.norm(dim=3)\n",
        "#         axes = axisangle / (angles.unsqueeze(-1) + 1e-7)\n",
        "\n",
        "#         # Implement the matrix from [1] with an additional identity fourth dimension\n",
        "#         # [1]: https://en.wikipedia.org/wiki/Transformation_matrix#Rotation_2\n",
        "\n",
        "#         angles_cos = angles.cos()\n",
        "#         angles_sin = angles.sin()\n",
        "\n",
        "#         res = torch.zeros(n, h, w, 4, 4, device=axisangle.device)\n",
        "#         res[:,:,:,:3,:3] = (1 - angles_cos.view(n,h,w,1,1)) * (axes.unsqueeze(-2) * axes.unsqueeze(-1))\n",
        "\n",
        "#         res[:,:,:,0,0] += angles_cos\n",
        "#         res[:,:,:,1,1] += angles_cos\n",
        "#         res[:,:,:,2,2] += angles_cos\n",
        "\n",
        "#         sl = axes[:,:,:,0] * angles_sin\n",
        "#         sm = axes[:,:,:,1] * angles_sin\n",
        "#         sn = axes[:,:,:,2] * angles_sin\n",
        "\n",
        "#         res[:,:,:,0,1] -= sn\n",
        "#         res[:,:,:,1,0] += sn\n",
        "\n",
        "#         res[:,:,:,1,2] -= sl\n",
        "#         res[:,:,:,2,1] += sl\n",
        "\n",
        "#         res[:,:,:,2,0] -= sm\n",
        "#         res[:,:,:,0,2] += sm\n",
        "\n",
        "#         res[:,:,:,3,3] = 1.0\n",
        "\n",
        "#         return res\n",
        "\n",
        "#     def _transformation_from_translation(self, translation):\n",
        "#         n, h, w = translation.shape[:3]\n",
        "\n",
        "#         # Implement the matrix from [1] with an additional dimension\n",
        "#         # [1]: https://en.wikipedia.org/wiki/Transformation_matrix#Affine_transformations\n",
        "\n",
        "#         res = torch.zeros(n, h, w, 4, 4, device=translation.device)\n",
        "#         res[:,:,:,:3,3] = translation\n",
        "#         res[:,:,:,0,0] = 1.0\n",
        "#         res[:,:,:,1,1] = 1.0\n",
        "#         res[:,:,:,2,2] = 1.0\n",
        "#         res[:,:,:,3,3] = 1.0\n",
        "\n",
        "#         return res\n",
        "\n",
        "#     def forward(self, x, invert):\n",
        "#         x = self.encoder(x)\n",
        "#         x = x[-1]  # take only the feature map of the last layer ...\n",
        "\n",
        "#         x_axisangle, x_translation = self.decoder(x)  # ... and pass it through the decoder\n",
        "\n",
        "#         x_rotation = self._transformation_from_axisangle(x_axisangle)\n",
        "\n",
        "#         if not invert:\n",
        "#             x_translation = self._transformation_from_translation(x_translation)\n",
        "\n",
        "#             return x_translation @ x_rotation\n",
        "\n",
        "#         else:\n",
        "#             x_rotation = x_rotation.transpose(3, 4)\n",
        "#             x_translation = -x_translation\n",
        "\n",
        "#             x_translation = self._transformation_from_translation(x_translation)\n",
        "\n",
        "#             return x_rotation @ x_translation\n",
        "\n",
        "\n",
        "# class SGDepth(nn.Module):\n",
        "#     KEY_FRAME_CUR = ('color_aug', 0, 0)\n",
        "#     KEY_FRAME_PREV = ('color_aug', -1, 0)\n",
        "#     KEY_FRAME_NEXT = ('color_aug', 1, 0)\n",
        "\n",
        "#     def __init__(self, split_pos=1, num_layers=18, grad_scale_depth=0.95, grad_scale_seg=0.05,\n",
        "#                  weights_init='pretrained', resolutions_depth=1, num_layers_pose=18):\n",
        "\n",
        "#         super().__init__()\n",
        "\n",
        "#         # sgdepth allowed for five possible split positions.\n",
        "#         # The PartialDecoder developed as part of sgdepth\n",
        "#         # is a bit more flexible and allows splits to be\n",
        "#         # placed in between sgdepths splits.\n",
        "#         # As this class is meant to maximize compatibility\n",
        "#         # with sgdepth the line below translates between\n",
        "#         # the split position definitions.\n",
        "#         split_pos = max((2 * split_pos) - 1, 0)\n",
        "\n",
        "#         # The Depth and the Segmentation Network have a common (=shared)\n",
        "#         # Encoder (\"Feature Extractor\")\n",
        "#         self.common = SGDepthCommon(\n",
        "#             num_layers, split_pos, (grad_scale_depth, grad_scale_seg),\n",
        "#             weights_init == 'pretrained'\n",
        "#         )\n",
        "\n",
        "#         # While Depth and Seg Network have a shared Encoder,\n",
        "#         # each one has it's own Decoder\n",
        "#         self.depth = SGDepthDepth(self.common, resolutions_depth)\n",
        "#         self.seg = SGDepthSeg(self.common)\n",
        "\n",
        "#         # The Pose network has it's own Encoder (\"Feature Extractor\") and Decoder\n",
        "#         self.pose = SGDepthPose(\n",
        "#             num_layers_pose,\n",
        "#             weights_init == 'pretrained'\n",
        "#         )\n",
        "\n",
        "#     def _batch_pack(self, group):\n",
        "#         # Concatenate a list of tensors and remember how\n",
        "#         # to tear them apart again\n",
        "\n",
        "#         group = tuple(group)\n",
        "\n",
        "#         dims = tuple(b.shape[0] for b in group)  # dims = (DEFAULT_DEPTH_BATCH_SIZE, DEFAULT_SEG_BATCH_SIZE)\n",
        "#         group = torch.cat(group, dim=0)  # concatenate along the first axis, so along the batch axis\n",
        "\n",
        "#         return dims, group\n",
        "\n",
        "#     def _multi_batch_unpack(self, dims, *xs):\n",
        "#         xs = tuple(\n",
        "#             tuple(x.split(dims))\n",
        "#             for x in xs\n",
        "#         )\n",
        "\n",
        "#         # xs, as of now, is indexed like this:\n",
        "#         # xs[ENCODER_LAYER][DATASET_IDX], the lines below swap\n",
        "#         # this around to xs[DATASET_IDX][ENCODER_LAYER], so that\n",
        "#         # xs[DATASET_IDX] can be fed into the decoders.\n",
        "#         xs = tuple(zip(*xs))\n",
        "\n",
        "#         return xs\n",
        "\n",
        "#     def _check_purposes(self, dataset, purpose):\n",
        "#         # mytransforms.AddKeyValue is used in the loaders\n",
        "#         # to give each image a tuple of 'purposes'.\n",
        "#         # As of now these purposes can be 'depth' and 'segmentation'.\n",
        "#         # The torch DataLoader collates these per-image purposes\n",
        "#         # into list of them for each batch.\n",
        "#         # Check all purposes in this collated list for the requested\n",
        "#         # purpose (if you did not do anything wonky all purposes in a\n",
        "#         # batch should be equal),\n",
        "\n",
        "#         for purpose_field in dataset['purposes']:\n",
        "#             if purpose_field[0] == purpose:\n",
        "#                 return True\n",
        "\n",
        "#     def set_gradient_scales(self, depth, segmentation):\n",
        "#         self.common.set_gradient_scales(depth, segmentation)\n",
        "\n",
        "#     def get_gradient_scales(self):\n",
        "#         return self.common.get_gradient_scales()\n",
        "\n",
        "#     def forward(self, batch):\n",
        "#         # Stitch together all current input frames\n",
        "#         # in the input group. So that batch normalization\n",
        "#         # in the encoder is done over all datasets/domains.\n",
        "#         dims, x = self._batch_pack(\n",
        "#             dataset[self.KEY_FRAME_CUR]\n",
        "#             for dataset in batch\n",
        "#         )\n",
        "\n",
        "#         # Feed the stitched-together input tensor through\n",
        "#         # the common network part and generate two output\n",
        "#         # tuples that look exactly the same in the forward\n",
        "#         # pass, but scale gradients differently in the backward pass.\n",
        "#         x_depth, x_seg = self.common(x)\n",
        "\n",
        "#         # Cut the stitched-together tensors along the\n",
        "#         # dataset boundaries so further processing can\n",
        "#         # be performed on a per-dataset basis.\n",
        "#         # x[DATASET_IDX][ENCODER_LAYER]\n",
        "#         x_depth = self._multi_batch_unpack(dims, *x_depth)\n",
        "#         x_seg = self._multi_batch_unpack(dims, *x_seg)\n",
        "\n",
        "#         outputs = list(dict() for _ in batch)\n",
        "\n",
        "#         # All the way back in the loaders each dataset is assigned one or more 'purposes'.\n",
        "#         # For datasets with the 'depth' purpose set the outputs[DATASET_IDX] dict will be\n",
        "#         # populated with depth outputs.\n",
        "#         # Datasets with the 'segmentation' purpose are handled accordingly.\n",
        "#         # If the pose outputs are populated is dependant upon the presence of\n",
        "#         # ('color_aug', -1, 0)/('color_aug', 1, 0) keys in the Dataset.\n",
        "#         for idx, dataset in enumerate(batch):\n",
        "#             if self._check_purposes(dataset, 'depth'):\n",
        "#                 x = x_depth[idx]\n",
        "#                 x = self.depth(*x)\n",
        "#                 x = reversed(x)\n",
        "\n",
        "#                 for res, disp in enumerate(x):\n",
        "#                     outputs[idx]['disp', res] = disp\n",
        "\n",
        "#             if self._check_purposes(dataset, 'segmentation'):\n",
        "#                 x = x_seg[idx]\n",
        "#                 x = self.seg(*x)\n",
        "\n",
        "#                 outputs[idx]['segmentation_logits', 0] = x\n",
        "\n",
        "#             if self.KEY_FRAME_PREV in dataset:\n",
        "#                 frame_prev = dataset[self.KEY_FRAME_PREV]\n",
        "#                 frame_cur = dataset[self.KEY_FRAME_CUR]\n",
        "\n",
        "#                 # Concatenating joins the previous and the current frame\n",
        "#                 # tensors along the first axis/dimension,\n",
        "#                 # which is the axis for the color channel\n",
        "#                 frame_prev_cur = torch.cat((frame_prev, frame_cur), dim=1)\n",
        "\n",
        "#                 outputs[idx]['cam_T_cam', 0, -1] = self.pose(frame_prev_cur, invert=True)\n",
        "\n",
        "#             if self.KEY_FRAME_NEXT in dataset:\n",
        "#                 frame_cur = dataset[self.KEY_FRAME_CUR]\n",
        "#                 frame_next = dataset[self.KEY_FRAME_NEXT]\n",
        "\n",
        "#                 frame_cur_next = torch.cat((frame_cur, frame_next), 1)\n",
        "#                 outputs[idx]['cam_T_cam', 0, 1] = self.pose(frame_cur_next, invert=False)\n",
        "\n",
        "#         return tuple(outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxWIQEQ952jO"
      },
      "outputs": [],
      "source": [
        "# from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "\n",
        "\n",
        "# class ArgumentsBase(object):\n",
        "#     def __init__(self):\n",
        "#         self.ap = ArgumentParser(\n",
        "#             formatter_class=ArgumentDefaultsHelpFormatter\n",
        "#         )\n",
        "\n",
        "#     def running_args(self):\n",
        "#         self.ap.add_argument('--weights-detector', type=str, default='weights/yolov5m.pt', help='weights for the main yolo detector')\n",
        "#         self.ap.add_argument('--weights-sign'    , type=str, default='/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/SGDepth/weights/best_sign.pt', help='sign detector weights')\n",
        "#         self.ap.add_argument('--weights-light'    , type=str, default='/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/SGDepth/weights/light_RegNety002.pth', help='sign detector weights')\n",
        "#         self.ap.add_argument('--disp-detector', type=str, default='/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/SGDepth/weights/sgdepth_model.pth', help='disparity model weights')\n",
        "#         self.ap.add_argument('--culane-model', type=str, default='/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/SGDepth/weights/culane_model.pkl', help='Culane model')\n",
        "#         self.ap.add_argument('--video', type=str, default='videos/test4.mp4', help = 'The input video')\n",
        "#         self.ap.add_argument('--save', action= 'store_true', help = 'Saving the output video')\n",
        "#         self.ap.add_argument('--noshow', action= 'store_true', help =  'Do not Show the output frames')\n",
        "#         self.ap.add_argument('--frame-drop', type = int, default = 1 , help =  'Frame Drop for processing')\n",
        "#         self.ap.add_argument('--outputfps', type = int, default = 30 , help =  'Output Video FPS')\n",
        "#         self.ap.add_argument('--fps', action = 'store_true' , help =  'Show fps')\n",
        "#         self.ap.add_argument('--output-name', type = str ,default = 'output.mov' , help =  'Outputput video address')\n",
        "#         self.ap.add_argument('--mode', type = str, default = 'day', help = 'Choose theprocessing model (day, night)')\n",
        "#         self.ap.add_argument('--save-frames', action = 'store_true' , help =  'Saves individual Frames')\n",
        "\n",
        "\n",
        "#     def SGDepth_harness_init_system(self):\n",
        "#         self.ap.add_argument(\n",
        "#             '--sys-cpu', default=False, action='store_true',\n",
        "#             help='Disable Hardware acceleration'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--sys-num-workers', type=int, default=3,\n",
        "#             help='Number of worker processes to spawn per DataLoader'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--sys-best-effort-determinism', default=False, action='store_true',\n",
        "#             help='Try and make some parts of the training/validation deterministic'\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def SGDepth_harness_init_model(self):\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-num-layers', type=int, default=18, choices=(18, 34, 50, 101, 152),\n",
        "#             help='Number of ResNet Layers in the depth and segmentation encoder'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-num-layers-pose', type=int, default=18, choices=(18, 34, 50, 101, 152),\n",
        "#             help='Number of ResNet Layers in the pose encoder'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-split-pos', type=int, default=1, choices=(0, 1, 2, 3, 4),\n",
        "#             help='Position in the decoder to split from common to separate depth/segmentation decoders'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-depth-min', type=float, default=0.3,\n",
        "#             help='Depth Estimates are scaled according to this min/max',\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-depth-max', type=float, default=80.0,\n",
        "#             help='Depth Estimates are scaled according to this min/max',\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-depth-resolutions', type=int, default=4, choices=(1, 2, 3, 4),\n",
        "#             help='Number of depth resolutions to generate in the network'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-name', type=str, default='sgdepth_base',\n",
        "#             help='A nickname for this model'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-load', type=str, default=None,\n",
        "#             help='Load a model state from a state directory containing *.pth files'\n",
        "#         )\n",
        "\n",
        "#         self.ap.add_argument(\n",
        "#             '--model-disable-lr-loading', default=False, action='store_true',\n",
        "#             help='Do not load the learning rate scheduler if you load a checkpoint'\n",
        "#         )\n",
        "\n",
        "\n",
        "#     def _parse(self):\n",
        "#         return self.ap.parse_args()\n",
        "\n",
        "\n",
        "# class InferenceEvaluationArguments(ArgumentsBase):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.running_args()\n",
        "#         self.SGDepth_harness_init_system()\n",
        "#         self.SGDepth_harness_init_model()\n",
        "\n",
        "#     def parse(self):\n",
        "#         opt = self._parse()\n",
        "\n",
        "#         return opt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrVwWF4e55Ep"
      },
      "outputs": [],
      "source": [
        "# # YOLOv5 🚀 by Ultralytics, GPL-3.0 license\n",
        "# \"\"\"\n",
        "# PyTorch Hub models https://pytorch.org/hub/ultralytics_yolov5/\n",
        "\n",
        "# Usage:\n",
        "#     import torch\n",
        "#     model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "#     model = torch.hub.load('ultralytics/yolov5:master', 'custom', 'path/to/yolov5s.onnx')  # file from branch\n",
        "# \"\"\"\n",
        "\n",
        "# import torch\n",
        "\n",
        "\n",
        "# def _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n",
        "#     \"\"\"Creates or loads a YOLOv5 model\n",
        "\n",
        "#     Arguments:\n",
        "#         name (str): model name 'yolov5s' or path 'path/to/best.pt'\n",
        "#         pretrained (bool): load pretrained weights into the model\n",
        "#         channels (int): number of input channels\n",
        "#         classes (int): number of model classes\n",
        "#         autoshape (bool): apply YOLOv5 .autoshape() wrapper to model\n",
        "#         verbose (bool): print all information to screen\n",
        "#         device (str, torch.device, None): device to use for model parameters\n",
        "\n",
        "#     Returns:\n",
        "#         YOLOv5 model\n",
        "#     \"\"\"\n",
        "#     from pathlib import Path\n",
        "\n",
        "#     from models.common import AutoShape, DetectMultiBackend\n",
        "#     from models.yolo import Model\n",
        "#     from utils.downloads import attempt_download\n",
        "#     from utils.general import LOGGER, check_requirements, intersect_dicts, logging\n",
        "#     from utils.torch_utils import select_device\n",
        "\n",
        "#     if not verbose:\n",
        "#         LOGGER.setLevel(logging.WARNING)\n",
        "\n",
        "#     check_requirements(exclude=('tensorboard', 'thop', 'opencv-python'))\n",
        "#     name = Path(name)\n",
        "#     path = name.with_suffix('.pt') if name.suffix == '' and not name.is_dir() else name  # checkpoint path\n",
        "#     try:\n",
        "#         device = select_device(('0' if torch.cuda.is_available() else 'cpu') if device is None else device)\n",
        "\n",
        "#         if pretrained and channels == 3 and classes == 80:\n",
        "#             model = DetectMultiBackend(path, device=device)  # download/load FP32 model\n",
        "#             # model = models.experimental.attempt_load(path, map_location=device)  # download/load FP32 model\n",
        "#         else:\n",
        "#             cfg = list((Path(__file__).parent / 'models').rglob(f'{path.stem}.yaml'))[0]  # model.yaml path\n",
        "#             model = Model(cfg, channels, classes)  # create model\n",
        "#             if pretrained:\n",
        "#                 ckpt = torch.load(attempt_download(path), map_location=device)  # load\n",
        "#                 csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n",
        "#                 csd = intersect_dicts(csd, model.state_dict(), exclude=['anchors'])  # intersect\n",
        "#                 model.load_state_dict(csd, strict=False)  # load\n",
        "#                 if len(ckpt['model'].names) == classes:\n",
        "#                     model.names = ckpt['model'].names  # set class names attribute\n",
        "#         if autoshape:\n",
        "#             model = AutoShape(model)  # for file/URI/PIL/cv2/np inputs and NMS\n",
        "#         return model.to(device)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         help_url = 'https://github.com/ultralytics/yolov5/issues/36'\n",
        "#         s = f'{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.'\n",
        "#         raise Exception(s) from e\n",
        "\n",
        "\n",
        "# def custom(path='path/to/model.pt', autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5 custom or local model\n",
        "#     return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\n",
        "\n",
        "\n",
        "# def yolov5n(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-nano model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5n', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-small model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5s', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-medium model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5m', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-large model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5l', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-xlarge model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5x', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-nano-P6 model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5n6', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-small-P6 model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5s6', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-medium-P6 model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5m6', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-large-P6 model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5l6', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# def yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n",
        "#     # YOLOv5-xlarge-P6 model https://github.com/ultralytics/yolov5\n",
        "#     return _create('yolov5x6', pretrained, channels, classes, autoshape, _verbose, device)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     model = _create(name='yolov5s', pretrained=True, channels=3, classes=80, autoshape=True, verbose=True)\n",
        "#     # model = custom(path='path/to/model.pt')  # custom\n",
        "\n",
        "#     # Verify inference\n",
        "#     from pathlib import Path\n",
        "\n",
        "#     import numpy as np\n",
        "#     from PIL import Image\n",
        "\n",
        "#     from utils.general import cv2\n",
        "\n",
        "#     imgs = [\n",
        "#         'data/images/zidane.jpg',  # filename\n",
        "#         Path('data/images/zidane.jpg'),  # Path\n",
        "#         'https://ultralytics.com/images/zidane.jpg',  # URI\n",
        "#         cv2.imread('data/images/bus.jpg')[:, :, ::-1],  # OpenCV\n",
        "#         Image.open('data/images/bus.jpg'),  # PIL\n",
        "#         np.zeros((320, 640, 3))]  # numpy\n",
        "\n",
        "#     results = model(imgs, size=320)  # batched inference\n",
        "#     results.print()\n",
        "#     results.save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsrM1Cof6VdG"
      },
      "outputs": [],
      "source": [
        "# from elements.yolo import YOLO, YOLO_Sign\n",
        "# from elements.PINet import LaneDetection\n",
        "# from elements.SGD import SGDepth_Model\n",
        "# from elements.light_classifier import light_classifier\n",
        "# from elements.asset import plot_object_colors, depth_estimator, apply_mask, apply_all_mask, ROI, plot_one_box, ui, horiz_lines\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "# from time import time as t\n",
        "# import sys\n",
        "# from datetime import timedelta\n",
        "# from SGDepth.arguments import InferenceEvaluationArguments\n",
        "\n",
        "\n",
        "# opt = InferenceEvaluationArguments().parse()\n",
        "\n",
        "\n",
        "# if opt.noshow and not opt.save:\n",
        "#     print(\"You're not getting any outputs!!\\nExit\")\n",
        "#     sys.exit()\n",
        "\n",
        "# # Load Models\n",
        "# detector = YOLO()\n",
        "# sign_detector = YOLO_Sign(opt.weights_sign)\n",
        "# light_detector = light_classifier(opt.weights_light)\n",
        "# lane_detector = LaneDetection(opt.culane_model)\n",
        "# depth_seg_estimator = SGDepth_Model(opt.disp_detector)\n",
        "\n",
        "\n",
        "# # Video Writer\n",
        "# cap = cv2.VideoCapture(\"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/SGDepth/videos/test7.mp4\")\n",
        "# frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "# w = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "# h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "\n",
        "# resize = not ((w == 1280) and (h == 720))\n",
        "\n",
        "\n",
        "# if opt.save:\n",
        "#     if len(opt.output_name.split('.'))==1:\n",
        "#         opt.output_name += '.mp4'\n",
        "#     output_video_folder = os.path.join('outputs/', opt.output_name.split('.')[0])\n",
        "#     if opt.save_frames:\n",
        "#         output_frames_folder = os.path.join(output_video_folder, 'frames')\n",
        "#         os.makedirs(output_frames_folder, exist_ok=True)\n",
        "#     output_video_name = os.path.join(output_video_folder, opt.output_name)\n",
        "#     os.makedirs(output_video_folder, exist_ok = True)\n",
        "#     print(output_video_folder)\n",
        "#     w = cap.get(cv2.CAP_PROP_FRAME_WIDTH) + 280\n",
        "#     h = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "#     out = cv2.VideoWriter(output_video_name,\n",
        "#                             cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "#                             opt.outputfps, (int(w), int(h)))\n",
        "\n",
        "# # Create color palettes for visualization\n",
        "# obj_colors, sign_colors = plot_object_colors()\n",
        "\n",
        "# frame_num = 0\n",
        "\n",
        "# total_fps = []\n",
        "# while(cap.isOpened()):\n",
        "\n",
        "#     ret, frame = cap.read()\n",
        "#     frame_num += 1\n",
        "#     if not frame_num % opt.frame_drop == 0:\n",
        "#         continue\n",
        "\n",
        "#     if ret:\n",
        "#         tc = t() # Start Time\n",
        "\n",
        "#         if resize:\n",
        "#             frame = cv2.resize(frame , (int(1280),int(720)))\n",
        "\n",
        "#         main_frame = frame.copy()\n",
        "#         yoloOutput = detector.detect(frame)\n",
        "#         signOutput = sign_detector.detect_sign(frame)\n",
        "#         depth, seg_img = depth_seg_estimator.inference(frame)\n",
        "\n",
        "\n",
        "#         # # Dynamic ROI Generation\n",
        "#         masked_image = ROI(main_frame, seg_img)\n",
        "\n",
        "\n",
        "#         # ### Sidewalk detection ###\n",
        "#         if opt.mode != 'night':\n",
        "#             frame = apply_mask(frame, seg_img, mode = opt.mode)\n",
        "\n",
        "\n",
        "#         # ### Lane Detection ###\n",
        "#         frame = lane_detector.detect_lane(frame, masked_image)\n",
        "\n",
        "\n",
        "#         # ### Object Detection ###\n",
        "#         depth_values = []\n",
        "#         for obj in yoloOutput.values:\n",
        "#             xyxy = [int(obj[0]), int(obj[1]), int(obj[2]), int(obj[3])] # [Xmin, Ymin, Xmax, Ymax]\n",
        "\n",
        "#             if obj[-2] in ['traffic light', 'stop sign', 'pedestrian'] :\n",
        "#                 plot_one_box(xyxy, frame, label=obj[-2], color=obj_colors[obj[-2]], line_thickness=3)\n",
        "#             else:\n",
        "#                 ### Distance Measurement ###\n",
        "#                 obj_area = (xyxy[3] - xyxy[1]) * (xyxy[2] - xyxy[0])\n",
        "#                 if obj_area > 6000:\n",
        "#                     depth_value = depth_estimator(xyxy, depth=depth, seg=seg_img, obj_name=obj[-1], mask_state=False)\n",
        "#                     depth_values.append(depth_value)\n",
        "#                     plot_one_box(xyxy, frame, distance=depth_value, label=obj[-2], color=obj_colors[obj[-2]], line_thickness=3)\n",
        "#                 else:\n",
        "#                     plot_one_box(xyxy, frame, label=obj[-2], color=obj_colors[obj[-2]], line_thickness=3)\n",
        "\n",
        "\n",
        "#         ### Sign Detection ###\n",
        "#         for sign in signOutput.values:\n",
        "#             xyxy = [sign[0], sign[1], sign[2], sign[3]]\n",
        "#             plot_one_box(xyxy, frame, label=sign[-1],  color=sign_colors[sign[-1]], line_thickness=3)\n",
        "\n",
        "#         # ### Cross Walk Lines ###\n",
        "#         # # frame = horiz_lines(main_frame, frame, mode = opt.mode)\n",
        "\n",
        "#         ### UI ###\n",
        "#         ui_bg = ui(main_frame, yoloOutput, light_detector, signOutput, depth_values)\n",
        "#         frame = cv2.hconcat([frame, ui_bg])\n",
        "\n",
        "\n",
        "#         t2 = t() # End of frame time\n",
        "#         fps = (1/(t2-tc))\n",
        "#         avg_fps = np.round(fps , 3)\n",
        "#         estimated_time = (frame_count - frame_num) / avg_fps\n",
        "#         estimated_time = str(timedelta(seconds=estimated_time)).split('.')[0]\n",
        "#         s = \"FPS : \"+ str(fps)\n",
        "#         if opt.fps:\n",
        "#             cv2.putText(frame, s, (40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), thickness= 2)\n",
        "\n",
        "\n",
        "#         # Saving the output\n",
        "#         if opt.save:\n",
        "#             out.write(frame)\n",
        "#             if opt.save_frames:\n",
        "#                 cv2.imwrite(os.path.join(output_frames_folder , '{0:04d}.jpg'.format(int(frame_num))) , frame)\n",
        "\n",
        "\n",
        "#         if not opt.noshow:\n",
        "#             cv2.imshow('frame', frame)\n",
        "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#                 break\n",
        "#     else:\n",
        "#         break\n",
        "\n",
        "#     sys.stdout.write(\n",
        "#         \"\\r[Input Video : %s] [%d/%d Frames Processed] [FPS : %f] [ET : %s]\"\n",
        "#         % (\n",
        "#             opt.video,\n",
        "#             frame_num,\n",
        "#             frame_count,\n",
        "#             fps,\n",
        "#             estimated_time\n",
        "#         )\n",
        "#     )\n",
        "\n",
        "# cap.release()\n",
        "\n",
        "# if not opt.noshow:\n",
        "#     cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysQQiU-DCuYv"
      },
      "outputs": [],
      "source": [
        "!cd /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYbUkHCjYL4a"
      },
      "source": [
        "## Download the weights and test videos and unzip the zip files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJnIiOiUB6UO"
      },
      "source": [
        "Both are anonymous links to download the weights and test videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmBv9y-_VQa9",
        "outputId": "b1c6110b-9e43-493d-c246-b1cfbb4b0fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1X1uKaGENEBZamF6tOfx9eKLTIQLsBN5h\n",
            "To: /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/weights.zip\n",
            "100% 146M/146M [00:01<00:00, 78.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-bRFhDt5EZULnQaKO35U3oX-p6yZwteB\n",
            "To: /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/videos.zip\n",
            "100% 235M/235M [00:03<00:00, 71.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1X1uKaGENEBZamF6tOfx9eKLTIQLsBN5h\n",
        "!gdown 1-bRFhDt5EZULnQaKO35U3oX-p6yZwteB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fUE5yNqeKvV",
        "outputId": "4bfcc9f6-fd1b-4566-ff27-77f91f1d314a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/videos.zip\n",
            "replace videos/test1.mp4? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  /content/drive/MyDrive/weights.zip\n",
            "replace weights/best_sign.pt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/videos.zip\n",
        "!unzip /content/drive/MyDrive/weights.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNNaaZhEKX2S",
        "outputId": "65d6f986-ad75-4971-f46a-070ff426ebd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPTNPSHzB6UO"
      },
      "source": [
        "1) **Local:** video test[1-7].mp4\n",
        "- **day:** videos test[1-5].mp4\n",
        "- **night:** videos test[6, 7].mp4\n",
        "\n",
        "2) **BDD100K:**\n",
        "- **day:** videos test[8-10-11].mp4\n",
        "- **night:** videos test[9].mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnLQMutDYTwh"
      },
      "source": [
        "## Run!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhLkmAJAB6UP"
      },
      "source": [
        "- **--video**       [The input video]\n",
        "- **--noshow**      [Do not Show the output frames]\n",
        "- **--save**        [Saving the output video]\n",
        "- **--output-name** [Outputput video name]\n",
        "- **--mode** ['night', if you want to test the night test videos. default: 'day']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch ultralytics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skmgqhZutzh3",
        "outputId": "48141749-d2ad-49c7-a11c-49c861faf17d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.131-py3-none-any.whl (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.9/626.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.27.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.40.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz1N-AVODSx8",
        "outputId": "98c91c84-a432-4039-9b18-566afd75bd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.16.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzM0jKPBWB9n",
        "outputId": "74ec88da-ca29-44e0-a8a9-61c055da7f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt not found, attempting AutoUpdate...\n",
            "Collecting gitpython>=3.1.30\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 13.6 MB/s eta 0:00:00\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 134.3 MB/s eta 0:00:00\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.31 smmap-5.0.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 7.2s, installed 1 package: /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "Yolov5 model loaded!\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m /usr/local/lib/python3.10/dist-packages/requirements.txt not found, check failed.\n",
            "YOLOv5 🚀 2023-7-8 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "newYOLOv5s summary: 213 layers, 7047883 parameters, 0 gradients\n",
            "Adding AutoShape... \n",
            "Sign model loaded!\n",
            "Traffic light classifier loaded!\n",
            "CULane model loaded!\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 198MB/s]\n",
            "SGDepth model loaded!\n",
            "outputs/samveg_shah\n",
            "[Input Video : /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/videos/test8.mp4] [82/1205 Frames Processed] [FPS : 3.327870] [ET : 0:05:37]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/main.py\", line 91, in <module>\n",
            "    frame = lane_detector.detect_lane(frame, masked_image)\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/elements/PINet.py\", line 40, in detect_lane\n",
            "    _, _, ti = self.test(np.array([frame]), org_frame, mask, self.threshold_point) \n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/elements/PINet.py\", line 56, in test\n",
            "    result = self.predict_lanes_test(test_images)\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/elements/PINet.py\", line 49, in predict_lanes_test\n",
            "    outputs, _ = self.lane_agent(inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/PINet/hourglass_network.py\", line 26, in forward\n",
            "    result2, out, feature2 = self.layer2(out)   \n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/PINet/util_hourglass.py\", line 296, in forward\n",
            "    out_confidence = self.out_confidence(outputs_a)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/PINet/util_hourglass.py\", line 169, in forward\n",
            "    outputs = self.conv1(inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/PINet/util_hourglass.py\", line 26, in forward\n",
            "    outputs = self.cbr_unit(inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2450, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/Autonomous-Vehicle-Environment-Perception/main.py --video /content/drive/MyDrive/Autonomous-Vehicle-Environment-Perception/videos/test8.mp4 --noshow --save --output-name 'samveg_shah'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSgtDYMLYp-_"
      },
      "source": [
        "The output video has been saved in **./outputs/myout/myout.mp4**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "c33ef0d4398937ecb5268daa690e9c83c694125b3e6c89d9a5ef925d0e1e9cba"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}